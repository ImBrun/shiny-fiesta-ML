{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a01e134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------Imports--------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as optimize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3c36411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "(10000,)\n",
      "(5000, 784)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "#-----------Load Dataset-----------\n",
    "\n",
    "path_train = \"fashion_train.npy\"\n",
    "path_test = \"fashion_test.npy\"\n",
    "\n",
    "train = np.load(path_train)\n",
    "test = np.load(path_test)\n",
    "\n",
    "#Split the training and test data into features and labels\n",
    "X_train = train[:,:784]\n",
    "y_train = train[:,784]\n",
    "\n",
    "X_test = test[:,:784]\n",
    "y_test = test[:,784]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61060e68",
   "metadata": {},
   "source": [
    "## LDA\n",
    "\n",
    "We will be using the first 2 linear discriminant variables as features for the Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f9e3382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LDA_components(X,y):\n",
    "    \n",
    "    def get_Sw(X, y):\n",
    "    \n",
    "        N = X.shape[1] #number of features\n",
    "        S_w = np.zeros((N,N))\n",
    "        class_labels = np.unique(y)\n",
    "        c = class_labels.shape[0] #number of classes\n",
    "\n",
    "        #calculate scatter matrix for each class\n",
    "        for class_ in range(c):\n",
    "\n",
    "            S_i = np.zeros((N,N))\n",
    "            class_subset = X[y == class_] #get rows which are a part of the current class\n",
    "            mean_vector = (np.mean(class_subset, axis=0)).reshape(N, 1) #vector m_i containing\n",
    "            #means of all features in class i\n",
    "\n",
    "            for row_idx in range(class_subset.shape[0]):\n",
    "\n",
    "                x = (class_subset[row_idx, :]).reshape(N, 1)\n",
    "                S_i += (np.dot((x - mean_vector), np.transpose(x - mean_vector))) #apply formula for within class scatter matrix\n",
    "\n",
    "            S_w += S_i\n",
    "\n",
    "        return S_w\n",
    "    #--------------Compute Between Class Scatter Matrix---------------\n",
    "    def get_Sb(X, y):\n",
    "    \n",
    "        N = X.shape[1] #number of features\n",
    "        m = (np.mean(X, axis=0)).reshape(N,1) #overall mean\n",
    "        S_b = np.zeros((N,N))\n",
    "        class_labels = np.unique(y)\n",
    "        c = class_labels.shape[0] #number of classes\n",
    "\n",
    "        for class_ in range(c):\n",
    "\n",
    "            class_subset = X[y == class_]\n",
    "            n_rows = class_subset.shape[0] #get number of rows which are a part of the current class\n",
    "            mean_vector = (np.mean(class_subset, axis=0)).reshape(N, 1) #vector m_i containing\n",
    "            #means of all features in class i\n",
    "            S_b += n_rows * ((mean_vector - m).dot((mean_vector - m).T)) #apply formula for between class scatter matrix\n",
    "\n",
    "        return S_b\n",
    "    \n",
    "    def get_linear_discriminants(S_w, S_b):\n",
    "    \n",
    "        # calculate the eigenvectors and eigenvalues of the matrix ((S_w)^-1)(S_b)\n",
    "        eig_vals, eig_vecs = np.linalg.eig((np.linalg.inv(S_w)).dot(S_b))\n",
    "\n",
    "        eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))] #create a list of corresponding\n",
    "        #eigenvectors and eigenvalues\n",
    "        eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True)\n",
    "        #sort the list by the eigenvalues in decreasing order\n",
    "\n",
    "        return eig_pairs\n",
    "    \n",
    "    N = X.shape[1] #get number of features\n",
    "    S_w = get_Sw(X, y) #get within class scatter matrix\n",
    "    S_b = get_Sb(X, y) #get between class scatter matrix\n",
    "    \n",
    "    sorted_eigenvecs = get_linear_discriminants(S_w, S_b) #get linear discriminants sorted by\n",
    "    #variance explained in descending order (most descriptive first)\n",
    "    \n",
    "    #get first 2 linear discriminants\n",
    "    W = np.hstack((sorted_eigenvecs[0][1].reshape(N,1), sorted_eigenvecs[1][1].reshape(N,1)))\n",
    "    \n",
    "    #transform the samples onto the new subspace\n",
    "    #transformed = X.dot(W)\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c54446b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = get_LDA_components(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b1e6361",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lda = X_train.dot(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc6e5ba",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1326d545",
   "metadata": {},
   "source": [
    "### Priors\n",
    "\n",
    "Get probability of a sample being from a certain class from the distribution of classes in the training set<br><br>\n",
    "\n",
    "To calculate the class priors, we'll be using the following formula:<br>\n",
    "\n",
    "$$p(C_k) = \\frac{n_k}{n}$$<br>\n",
    "\n",
    "Where:<br>\n",
    "$p(C_k)$ - prior probability<br>\n",
    "$n_k$ - number of training samples of a particular class<br>\n",
    "$n$ - total number of training samples<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57965e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_priors(X, y):\n",
    "    \n",
    "    n_rows = X.shape[0]\n",
    "    classes, counts = np.unique(y, return_counts=True) # return how many samples there are of each class\n",
    "    \n",
    "    # prior probability of a sample being from a certain class\n",
    "    priors = {class_: count / n_rows for class_, count in zip(classes, counts)}\n",
    "    \n",
    "    return priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a0ce134",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = get_priors(X_lda, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78d44840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.2033, 1: 0.1947, 2: 0.2001, 3: 0.2005, 4: 0.2014}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e9ba7c",
   "metadata": {},
   "source": [
    "## Feature PMF\n",
    "\n",
    "Get probability of a feature being in different value ranges.<br>\n",
    "To estimate these probabilities we use histograms returning the value range (calculated based on the hyperparameter h - the number of bins) and the number of samples which contain the corresponding feature in that range - so we divide this sample count by the total number of samples to calculate the probability.<br>\n",
    "We will be using this function to calculate the conditional probabilities, finding the PMF of each feature given that it belongs to each of the possible classes.<br><br>\n",
    "\n",
    "Note: It's possible that some resulting bins may be empty if there are no samples with the corresponding feature in the value range of that bin - case in which the probability of that value range will be 0.<br>\n",
    "This means that if any sample has the corresponding feature in that value range with an empty bin it will be immediately disqualified from being from that class because the conditional probability of that feature will be 0, so the product of conditional probabilities will be 0. To combat this we added a small number instead of a flat 0 - so an unlikely feature is still heavily penalized, but not immediately disqualified from being from that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf76b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pmf_hist(X, h):\n",
    "\n",
    "    # return number of samples in each bin and edge values of the bin\n",
    "    counts, edges = np.histogram(X, bins=h)\n",
    "    \n",
    "    # list of probabilities of a feature being from a certain value range\n",
    "    cond = []\n",
    "    \n",
    "    # total number of samples\n",
    "    total = np.sum(counts)\n",
    "    \n",
    "    for upper_bound, c in zip(edges[1:], counts):\n",
    "        \n",
    "        # get percentage of samples in bin and append ending point of the bin\n",
    "        # and percentage to the conditionals list\n",
    "        if c != 0:\n",
    "            curr_cond = c / total\n",
    "        \n",
    "        # if a bin is not assigned any points (i.e there were no features in that value range)\n",
    "        # assign it a very very low probability rather than a flat 0\n",
    "        # chose to do this because a single PIXEL not in a yet encountered value range for that\n",
    "        # feature for the given class immediately disqualifies that picture being from that class\n",
    "        # otherwise, so instead give it a big penalty without immediately disqualifying it\n",
    "        else:\n",
    "            curr_cond = 0.001\n",
    "            \n",
    "        cond.append((upper_bound.real, curr_cond))\n",
    "        \n",
    "    return cond"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67152db9",
   "metadata": {},
   "source": [
    "## Conditional probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ad255",
   "metadata": {},
   "source": [
    "Calculate the probability of each feature being in a certain value range given that it comes from each of the possible classes.<br><br>\n",
    "We are returning them as a dictionary where each of the possible classes is a key, and the value of each of those classes is another dictionary, where each of the features(their index) is a key, and each of their values is a list containing tuples corresponding to each of the value intervals' upper bound, and the probability of the feature being in that range, given that it comes from its corresponding class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c5bf772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conditionals(X, y, h):\n",
    "    \n",
    "    N = X.shape[1] # number of features\n",
    "    class_labels = np.unique(y)\n",
    "    c = class_labels.shape[0] # number of classes\n",
    "    \n",
    "    # get conditional probabilities of features given the sample is from a certain class\n",
    "    conditionals = {class_ : {} for class_ in class_labels}\n",
    "    \n",
    "    for class_ in range(c):\n",
    "        \n",
    "        # get features and labels corresponding to samples from the current class\n",
    "        rows_subset = X[y == class_]\n",
    "        \n",
    "        for feature in range(N):\n",
    "            \n",
    "            features_subset = rows_subset[:, feature] # get vector of values of a certain feature in all samples\n",
    "                                                      # of the current class\n",
    "            \n",
    "            # get probabilities of feature value ranges given the current class\n",
    "            estimate = estimate_pmf_hist(features_subset, h)\n",
    "            \n",
    "            # Note: this should never happen if we cleaned the data set, but just in case for debugging\n",
    "            if estimate == 0 or estimate == None:\n",
    "                print(f\"no bins at feature {feature} in class {class_}\")\n",
    "                \n",
    "            conditionals[class_][feature] = estimate\n",
    "            \n",
    "    return conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0eb9dad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrei/Library/Python/3.9/lib/python/site-packages/numpy/lib/histograms.py:841: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  indices = f_indices.astype(np.intp)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {0: [(-5.95747735381325, 0.000983767830791933),\n",
       "   (-3.626502487514723, 0.025086079685194294),\n",
       "   (-1.2955276212161966, 0.49188391539596654),\n",
       "   (1.035447245082329, 0.43433349729463844),\n",
       "   (3.366422111380855, 0.04771273979340875)],\n",
       "  1: [(-2.4288518255313014, 0.010329562223315297),\n",
       "   (-0.41876769278369075, 0.15937038858829317),\n",
       "   (1.5913164399639195, 0.6527299557304476),\n",
       "   (3.6014005727115306, 0.16969995081160846),\n",
       "   (5.611484705459141, 0.007870142646335464)]},\n",
       " 1: {0: [(-0.5129972532639577, 0.004622496147919877),\n",
       "   (4.041636692065667, 0.025166923472008218),\n",
       "   (8.596270637395293, 0.11248073959938366),\n",
       "   (13.150904582724916, 0.6373908577298408),\n",
       "   (17.70553852805454, 0.22033898305084745)],\n",
       "  1: [(-4.694712491619638, 0.007704160246533128),\n",
       "   (-2.4261021583731903, 0.06420133538777606),\n",
       "   (-0.1574918251267423, 0.8294812532100667),\n",
       "   (2.1111185081197057, 0.08269131997945557),\n",
       "   (4.379728841366154, 0.015921931176168466)]},\n",
       " 2: {0: [(-4.896874831185922, 0.004997501249375313),\n",
       "   (-2.4381190510291235, 0.22538730634682658),\n",
       "   (0.020636729127675224, 0.7226386806596702),\n",
       "   (2.479392509284475, 0.044977511244377814),\n",
       "   (4.938148289441273, 0.001999000499750125)],\n",
       "  1: [(-5.9997997689580895, 0.06496751624187906),\n",
       "   (-3.666415383029147, 0.56071964017991),\n",
       "   (-1.3330309971002041, 0.3268365817091454),\n",
       "   (1.0003533888287386, 0.04147926036981509),\n",
       "   (3.3337377747576817, 0.005997001499250375)]},\n",
       " 3: {0: [(-4.152399639777039, 0.004987531172069825),\n",
       "   (-1.3218578090111475, 0.0882793017456359),\n",
       "   (1.5086840217547444, 0.6618453865336659),\n",
       "   (4.339225852520636, 0.23491271820448878),\n",
       "   (7.169767683286526, 0.00997506234413965)],\n",
       "  1: [(-3.0565294723952303, 0.000997506234413965),\n",
       "   (-0.32221503703133525, 0.023940149625935162),\n",
       "   (2.4120993983325594, 0.21296758104738153),\n",
       "   (5.146413833696455, 0.5625935162094763),\n",
       "   (7.880728269060348, 0.19950124688279303)]},\n",
       " 4: {0: [(-4.994804775186123, 0.004965243296921549),\n",
       "   (-2.444041738512552, 0.29890764647467727),\n",
       "   (0.10672129816101883, 0.6355511420059583),\n",
       "   (2.6574843348345896, 0.05809334657398212),\n",
       "   (5.208247371508159, 0.0024826216484607746)],\n",
       "  1: [(-4.981348964120645, 0.026812313803376366),\n",
       "   (-2.5696295095933035, 0.26266137040714993),\n",
       "   (-0.1579100550659609, 0.46772591857000995),\n",
       "   (2.253809399461381, 0.21400198609731877),\n",
       "   (4.665528853988722, 0.028798411122144985)]}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_bins = 5\n",
    "conditionals_lda = get_conditionals(X_lda, y_train, n_bins)\n",
    "conditionals_lda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a6c36",
   "metadata": {},
   "source": [
    "## Predict probabilities\n",
    "\n",
    "For each sample, predict the probability of that sample being from each of the possible classes.<br><br>\n",
    "To do so, for each class, we'll look at the probability of each of the features belonging to that class (the probability of that feature being in the value range it is given that it's from a certain class), and we'll take the product of those probabilities as the probability that our observed sample is from that class - this assumes that the features are all independent, which is why it's \"naive\".<br>\n",
    "These are the class conditional probabilities, calculated with the following formula:<br>\n",
    "$$p(C_k | x) \\approx \\prod_{j}^{m}p(x_j | C_k)$$<br>\n",
    "\n",
    "Where:<br>\n",
    "$p(C_k | x)$ - probability of sample being from a particular class given its features<br>\n",
    "$p(x_j | C_k)$ - probability of a certain feature given that it comes from a certain class\n",
    "\n",
    "After we have the probability of a sample being from each of the classes, we will also normalize those probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96b23418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probabilities(X, cond, priors):\n",
    "    \n",
    "    N = X.shape[1] #number of features\n",
    "    n_rows = X.shape[0]\n",
    "    c = len(priors) #number of classes\n",
    "    \n",
    "    # matrix of class probabilities for each sample\n",
    "    probs = np.zeros((n_rows, c))\n",
    "    \n",
    "    # for each sample\n",
    "    for i in range(n_rows):\n",
    "        \n",
    "        # current sample\n",
    "        x = X[i]\n",
    "        \n",
    "        # vector of class probabilities for the current sample\n",
    "        class_probs = np.zeros((c, 1))\n",
    "        \n",
    "        # for each class\n",
    "        for class_ in range(c):\n",
    "            \n",
    "            # get class prior\n",
    "            prior = priors[class_]\n",
    "            \n",
    "            # initialize the probability of the sample belonging to that class\n",
    "            # as the prior of that class\n",
    "            class_prob = prior\n",
    "            \n",
    "            # for each feature\n",
    "            for j in range(N):\n",
    "                \n",
    "                feature = x[j]\n",
    "                \n",
    "                # get probability of current feature\n",
    "                # if it doesn't exist in the conditionals dictionary return a very very small probability\n",
    "                curr_pmf = cond[class_][j]\n",
    "                \n",
    "                feature_prob = 0.001\n",
    "                \n",
    "                # find value interval in which the feature is and assign it\n",
    "                # the conditional probability corresponding to that interval\n",
    "                for upper_bound, cond_prob in curr_pmf:\n",
    "                    \n",
    "                    if upper_bound < feature:\n",
    "                        continue\n",
    "                    else:\n",
    "                        feature_prob = cond_prob\n",
    "                        break\n",
    "                    \n",
    "                # the probability of the sample belonging to the current class\n",
    "                # is the probability of all of the feature values \n",
    "                class_prob *= feature_prob\n",
    "                \n",
    "                \n",
    "            class_probs[class_] = class_prob\n",
    "           \n",
    "        # normalize class probabilities\n",
    "        \n",
    "        # to avoid NaN values, all the probabilities will remain the same\n",
    "        if np.all(class_probs, 0):\n",
    "            #print(f\"found all probabilities 0 at image {i}\")\n",
    "            class_probs += 1\n",
    "            \n",
    "        total = np.sum(class_probs)\n",
    "        \n",
    "        posterior_probs = class_probs / total\n",
    "        \n",
    "        probs[i] = posterior_probs.reshape((5,))\n",
    "        \n",
    "    \n",
    "    return probs\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15f3db",
   "metadata": {},
   "source": [
    "## Predict class\n",
    "\n",
    "Predict that each sample belongs to the class with the highest class conditional probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f463138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(X, cond, priors):\n",
    "    \n",
    "    probs = predict_probabilities(X, cond, priors)\n",
    "    \n",
    "    # return class with highest probability for each sample\n",
    "    return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc1ee6c",
   "metadata": {},
   "source": [
    "## Test Naive Bayes performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0ac107",
   "metadata": {},
   "source": [
    "### Training set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eba47a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 5\n",
    "conditionals_lda = get_conditionals(X_lda, y_train, n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1573239",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lda = predict_class(X_lda, conditionals_lda, priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd0a2e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7598"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.sum(predictions_lda == y_train))/(X_lda.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a51e26",
   "metadata": {},
   "source": [
    "### Test set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64cacfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lda_test = X_test.dot(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3041df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lda_test = predict_class(X_lda_test, conditionals_lda, priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "657fb8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7098"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.sum(predictions_lda_test == y_test))/(X_lda_test.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
